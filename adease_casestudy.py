# -*- coding: utf-8 -*-
"""AdEase_Casestudy.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/Rashida-Rangwala/AdEase_Casestudy/blob/main/AdEase_Casestudy.ipynb

# **Ad Ease**

Ad Ease is an ads and marketing based company helping businesses elicit maximum clicks @ minimum cost. AdEase is an ad infrastructure to help businesses promote themselves easily, effectively, and economically. The interplay of 3 AI modules - Design, Dispense, and Decipher, come together to make it this an end-to-end 3 step process digital advertising solution for all.

You are working in the Data Science team of Ad ease trying to understand the per page view report for different wikipedia pages for 550 days, and forecasting the number of views so that you can predict and optimize the ad placement for your clients. You are provided with the data of 145k wikipedia pages and daily view count for each of them. Your clients belong to different regions and need data on how their ads will perform on pages in different languages.
"""

!pip install --upgrade --no-cache-dir gdown

import pandas as pd
import numpy as np
import pylab as p
import matplotlib.pyplot as plot
from collections import Counter
import re
import os
import seaborn as sns

import warnings
warnings.filterwarnings("ignore")
warnings.simplefilter("ignore")

sns.set(rc={'figure.figsize':(11.7,8.27)})

"""**Data Dictionary:**

There are two csv files given

**train_1.csv**: In the csv file, each row corresponds to a particular article and each column corresponds to a particular date. The values are the number of visits on that date.

The page name contains data in this format:

SPECIFIC NAME _ LANGUAGE.wikipedia.org _ ACCESS TYPE _ ACCESS ORIGIN

having information about the page name, the main domain, the device type used to access the page, and also the request origin(spider or browser agent)



**Exog_Campaign_eng**: This file contains data for the dates which had a campaign or significant event that could affect the views for that day. The data is just for pages in English.

There’s 1 for dates with campaigns and 0 for remaining dates. It is to be treated as an exogenous variable for models when training and forecasting data for pages in English
"""

import gdown
url='https://drive.google.com/file/d/1CJOMYyg64x3gN52p6OqypN6UUgDnUhkm/view?usp=sharing'

ider=url.split('/')[-2]
!gdown --id $ider

!gdown 1H9054-eVP9IdANPOblXwX7Nd2r_Sjf1u

exog = pd.read_csv("Exog_Campaign_eng")
exog.head()

exog.describe()

exog.info()

"""Exog data doesn't have any Nulls. We will use it later on when required while creating the ML model."""

train = pd.read_csv('new_train.csv')

train.shape

train.head()

train.info()

train.describe()

"""##Missing Value Treatment"""

train.isnull().sum()

days = [r for r in range(1, len(train.columns))]
plot.figure(figsize=(10,7))
plot.xlabel('Day')
plot.ylabel('Null values')
plot.plot(days, train.isnull().sum()[1:])

"""A

From the above plot it is evident that the no. of Null values reduce overtime. We can say that initially these pages were not created so they don't have any views.

We are dropping the rows having all NULL values first and then dropping the rows having more than 300 values as NULL becuase we have a total of 551 records, if we have more than 300 NULLs it means half of our data is NULL and that would not help in model creation.
"""

print(train.shape)
train=train.dropna(how='all')
#‘all’ : If all values are NA, drop that row or column.
print(train.shape)

train=train.dropna(thresh=300)
print(train.shape)

"""We haven't lost much data after the removal of NULLs so we can proceed with the cleaned data now.

Filling all the remaining NULLs with 0
"""

train=train.fillna(0)

days = [r for r in range(1, len(train.columns))]
plot.figure(figsize=(10,7))
plot.xlabel('Day')
plot.ylabel('Null values')
plot.plot(days, train.isnull().sum()[1:])

"""# **EDA**

The page name contains data in this format:

SPECIFIC NAME _ LANGUAGE.wikipedia.org _ ACCESS TYPE _ ACCESS ORIGIN

having information about the page name, the main domain, the device type used to access the page, and also the request origin(spider or browser agent)

We will split the data given in the page name column to find out the different parts of the data.
"""

def split_page(page):
  w = re.split('_|\.', page)
  return ' '.join(w[:-5]), w[-5], w[-2], w[-1]

li = list(train.Page.apply(lambda x: split_page(str(x))))
df = pd.DataFrame(li)
df.columns = ['Title', 'Language', 'Access_type','Access_origin']
df = pd.concat([train, df], axis = 1)

df.head()

"""We have got 4 new columns after spliting the Page column - Title, Language, Access_type, Access_origin"""

sns.countplot(df["Language"])

sns.countplot(df["Access_type"])

sns.countplot(df["Access_origin"])

"""Let's explore the languages more. We have found lots of records for "commons".
Let's check it and see if they can be further splitted into any language or not
"""

df[df["Language"] == "commons"]

df[df["Language"] == "www"]

# Checking another way of fetching the language out of the string
def lang(Page):
    val = re.search('[a-z][a-z].britanica.org',Page)
    if val:
        #print(val)
        #print(val[0][0:2] )

        return val[0][0:2]

    return 'no_lang'

df['Language']=df['Page'].apply(lambda x: lang(str(x)))

sns.countplot(df["Language"])

"""Based on the language data we have, let's create a dataframe for languages and store the mean of those date wise and check how each language performs"""

df_lang = df.groupby("Language").mean().transpose()
df_lang.head(10)

df_lang.reset_index(inplace=True)
df_lang.set_index('index', inplace=True)

df_lang.plot(figsize=(12,7))
plot.ylabel('Views per Page')

"""Looking at the above plot we can say that language - English is preferred over others. People view pages in English language way more than the rest.

Also there is a very interesting insight that there are some peaks in the data, especially if we look at 2016-08-04 in both en and ru language. To study about this in detail, we will look at the Exogenous data provided to us for en language.

Studying the patterns for Language - "en"
"""

df_en = df_lang["en"]
df_en.head()

df_en = df_lang["en"].to_frame().reset_index()

# Rename columns
df_en.columns = ["date", "views"]

df_en.set_index('date', inplace=True)

df_en.head()

# linear interpolation
df_en.views = df_en.views.interpolate(method='linear')

# anomalies - clip quantiles
df_en.views = df_en.views.clip(upper=df_en.views.quantile(0.98), lower=df_en.views.quantile(0.02))

# plot
df_en.views.plot(style='-o', figsize=(20,6))

from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
plot_acf(df_en.views);

plot_pacf(df_en.views);

"""Let's check if our TS is stationary or not with the help of Dickey Fuller Test"""

import statsmodels.api as sm
def adf_test(dataset):
   pvalue = sm.tsa.stattools.adfuller(dataset)[1]
   if pvalue <= 0.05:
      print('Sequence is stationary')
   else:
      print('Sequence is not stationary')

adf_test(df_en.views)

"""As the TS is not stationary let's use differencing to make it stationary"""

df_en_st = df_en.copy()
df_en_st.views = df_en_st.views.diff(1)
df_en_st.dropna(inplace=True)
adf_test(df_en_st.views)

"""Differencing has worked here. The TS is now stationary and can be used for forecasting."""

plot_acf(df_en_st.views);

plot_pacf(df_en_st.views);

"""# **ARIMA model**"""

train = df_en[:-20]
test = df_en[-20:]

import statsmodels.api as sm
train=df_en[:520]
test=df_en[520:]
model=sm.tsa.statespace.SARIMAX(train,order=(4, 1, 3))
results=model.fit()

fc=results.forecast(30,dynamic=True)

# Make as pandas series
fc_series = pd.Series(fc)
# Plot
train.index=train.index.astype('datetime64[ns]')
test.index=test.index.astype('datetime64[ns]')
plot.figure(figsize=(12,5), dpi=100)
plot.plot(train, label='training')
plot.plot(test, label='actual')
plot.plot(fc_series, label='forecast')

plot.title('Forecast vs Actuals')
plot.legend(loc='upper left', fontsize=8)

mape = np.mean(np.abs(fc - test.views)/np.abs(test.views))
rmse = np.mean((fc - test.views)**2)**.5
print("mape:",mape)
print("rsme:",rmse)

"""# **Using exog in SARIMAX**"""

ex=exog['Exog'].to_numpy()

train=df_en[:520]
test=df_en[520:]
model=sm.tsa.statespace.SARIMAX(train,order=(4, 1, 3),seasonal_order=(1,1,1,7),exog=ex[:520])
results=model.fit()

fc=results.forecast(30,dynamic=True,exog=pd.DataFrame(ex[520:]))

# Make as pandas series
fc_series = pd.Series(fc)
# Plot
train.index=train.index.astype('datetime64[ns]')
test.index=test.index.astype('datetime64[ns]')
plot.figure(figsize=(12,5), dpi=100)
plot.plot(train, label='training')
plot.plot(test, label='actual')
plot.plot(fc_series, label='forecast')

plot.title('Forecast vs Actuals')
plot.legend(loc='upper left', fontsize=8)

from sklearn.metrics import (
    mean_squared_error as mse,
    mean_absolute_error as mae,
    mean_absolute_percentage_error as mape
)

# Creating a function to print values of all these metrics.
def performance(actual, predicted):
    print('MAE :', round(mae(actual, predicted), 3))
    print('RMSE :', round(mse(actual, predicted)**0.5, 3))
    print('MAPE:', round(mape(actual, predicted), 3))

mape = np.mean(np.abs(fc - test.views)/np.abs(test.views))
rmse = np.mean((fc - test.views)**2)**.5
print("mape:",mape)
print("rsme:",rmse)

"""# **FB Prophet**"""

!pip install pystan~=2.14
!pip install fbprophet

df_en

exog

"""FB prophet without using exogenous variable"""

from fbprophet import Prophet
m = Prophet(weekly_seasonality=True)
m.fit(df2[['ds', 'y']][:-20])
future = m.make_future_dataframe(periods=20,freq="D")
forecast = m.predict(future)
fig = m.plot(forecast)

"""FB prophet with exogenous variable"""

model2=Prophet(interval_width=0.9, weekly_seasonality=True, changepoint_prior_scale=1)
model2.add_regressor('exog')
model2.fit(df2[:-20])
forecast2 = model2.predict(df2)
fig = model2.plot(forecast2)

y_true = df2['y'].values
y_pred = forecast2['yhat'].values

plot.plot(y_true, label='Actual')
plot.plot(y_pred, label='Predicted')
plot.legend()
plot.show()

mape = np.mean(np.abs(forecast2['yhat'][-20:] - df2['y'][-20:].values)/np.abs(df2['y'][-20:].values))
print("mape:",mape)

"""# **Conclusion**
SARIMAX is performing better in comparision to ARIMA or FB prophet.

We can easily see that there is Seasonality and Trend in the data.

Differencing of 1 lap is required in the data.

The value of PDQ and PDQS is choosen after multiple tries.

# **Recommendations based on MAPE & mean_visits:**
 English language is a clear winner. Maximum advertisement should be done on
 English pages. Their MAPE is low & mean visits are high.

 Chinese language has lowest number of visits. Advertisements on these pages
 should be avoided unless business has specific marketing strategy for Chinese
 populations.

 Russian language pages have decent number of visits and low MAPE. If used
 properly, these pages can result in maximum conversion.

 Spanish language has second highest number of visits but their MAPE is highest.
 There is a possibility advertisements on these pages won't reach the final people.

 French, German & Japenese have medium level of visits & medium MAPE levels.

 Depending on target customers advertisements should be run on these pages.

# **Questionnaire**

 1. Defining the problem statements and where can this and modifications of this be
 used?
 We are working in the Data Science team of Ad ease trying to understand the per
 page view report for different wikipedia pages for 550 days, and forecasting the number of views so that you can predict and optimize the ad placement for your clients. We are provided with the data of 145k wikipedia pages and daily view count for each of them. Our clients belong to different regions and need data on how their ads will perform on pages in different languages.
 By creating a proper forecasting model to predict the fluctuations of visits on pages,
 we can help the business team to optimise the marketing spend. If we can predict
 the days with higher visits properly, the business will run the ads for those specific
 days and still be able to reach wider audience with most optimized spend.



 2. Write 3 inferences you made from the data visualizations.

 There are 7 Languages found based on data provided. English has highest
 number of pages followed by Japense, German & French.
 There are 3 Access types : All-access(51.4%), mobile-web (24.9%) and
 desktop(23.6%).
 There are 2 Access-origins: all-agents (75.8%) and spider (24.2%).
 English language is a clear winner. Maximum advertisement should be done on
 English pages. Their MAPE is low & mean visits are high.
 Chinese language has lowest number of visits. Advertisements on these pages
 should be avoided unless business has specific marketing strategy for Chinese
 populations.
 Russian language pages have decent number of visits and low MAPE. If used
 properly, these pages can result in maximum conversion.
 Spanish language has second highest number of visits but their MAPE is highest.
 There is a possibility advertisements on these pages won't reach the final people.
 French, German & Japenese have medium level of visits & medium MAPE levels.
 Depending on target customers advertisements should be run on these pages.



 3. What does the decomposition of series do?

The decomposition of time series is a statistical task that deconstructs a time series
 into several components, each representing one of the underlying categories of
 patterns.
 There are two principal types of decomposition : Additive & Multiplicative.
 In present business case we have used Additive Model for deconstructing the time
 series.


 4. What level of differencing gave you a stationary series?

A non-stationary time series can be converted to a stationary time series through a
 technique called differencing. Differencing series is the change between consecutive
 data points in the series.

 In some cases, just differencing once will still yield a nonstationary time series. In
 that case a second order differencing is required.
 Seasonal differencing is the change between the same period in two different
 seasons. Assume a season has period, m
 -
yt yt−m
 Once the time series becomes stationary, no differencing is required.



 5. Difference between arima, sarima & sarimax.


 The ARIMA model is an ARMA model yet with a preprocessing step included in the
 model that we represent using I(d). I(d) is the difference order, which is the number
 of transformations needed to make the data stationary. So, an ARIMA model is
 simply an ARMA model on the differenced time series.

 In SARIMA models there is an additional set of autoregressive and moving average
 components.The additional lags are offset by the frequency of seasonality (ex. 12 —
 monthly, 24 — hourly). SARIMA models allow for differencing data by seasonal
 frequency, yet also by non-seasonal differencing.
 Equation of SARIMA model can be represented as below:
 SARIMAX model takes into account exogenous variables, or in other words, use
 external data in our forecast. Some real-world examples of exogenous variables
 include gold price, oil price, outdoor temperature, exchange rate.


 6. Compare the number of views in different languages


 Mean number of views (Popularity sequence) of various languages have the
 following :
 English > Spanish > Russian > German > Japenese > French > Chinese


 7. What other methods other than grid search would be suitable to get the model
 for all languages?


 Deep understanding of Domain / Business or relevant experience in the same
 field can be good starting point for estimating the parameters of the model
 intiuitavely.

 Second level estimation can come from ACF & PACF plots of the time series. We
 can take following steps for estimation of p, q, d:

 Test for stationarity using the augmented dickey fuller test.

 If the time series is stationary try to fit the ARMA model, and if the time series is
 non-stationary then seek the value of d.

 If the data is getting stationary then draw the autocorrelation and partial
 autocorrelation graph of the data.

 Draw a partial autocorrelation graph(ACF) of the data. This will help us in finding
 the value of p because the cut-off point to the PACF is p.

 Draw an autocorrelation graph(ACF) of the data. This will help us in finding the
 value of q because the cut-off point to the ACF is q
"""

